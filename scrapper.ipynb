{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to check if URL already exists in CSV\n",
    "def is_url_duplicate(csv_filename, url):\n",
    "    with open(csv_filename, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            if row['Link'] == url:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Function to check if no results message is present\n",
    "def no_results_present(driver):\n",
    "    try:\n",
    "        driver.find_element_by_class_name(\"search__no-results__message\")\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Function to check if row contains NaN\n",
    "def contains_nan(row):\n",
    "    for value in row.values():\n",
    "        if value == \"N/A\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Specify the number of pages to scrape\n",
    "num_pages = 25\n",
    "\n",
    "# Specify the path to chromedriver.exe (change it as per your directory)\n",
    "webdriver_service = Service(r\"C:\\chromedriver-win64\\chromedriver.exe\")\n",
    "\n",
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome(service=webdriver_service)\n",
    "\n",
    "# Create and open CSV file for writing\n",
    "csv_filename = 'output.csv'\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    # Define fieldnames for CSV\n",
    "    fieldnames = ['Headline', 'Description', 'Date', 'Link']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "    # Write headers to CSV\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Search terms related to business\n",
    "    search_terms = ['business_pakistan', 'business_asia', 'business_india', 'business_china', 'business_economy',\n",
    "                    'business_technology', 'business_startup', 'business_trade', 'business_finance', 'business_investment',\n",
    "                    'business_marketing', 'business_management', 'business_entrepreneurship', 'business_leadership', \n",
    "                    'business_strategy', 'business_global', 'business_corporate', 'business_government', 'business_environment',\n",
    "                    'business_sustainability', 'business_social', 'business_culture', 'business_trends', 'business_opportunity']\n",
    "    \n",
    "    # Initialize counter for the number of records written to CSV\n",
    "    record_counter = 0\n",
    "    \n",
    "    # Iterate through search terms\n",
    "    for search_term in search_terms:\n",
    "        for page_num in range(1, num_pages + 1):\n",
    "            # URL to scrape\n",
    "            url = f\"https://edition.cnn.com/search?q={search_term}&from={100 * (page_num - 1)}&size=100&page={page_num}&sort=newest&types=all&section=\"\n",
    "            \n",
    "            try:\n",
    "                # Load the page\n",
    "                driver.get(url)\n",
    "\n",
    "                # Wait for the content to load\n",
    "                wait = WebDriverWait(driver, 10)\n",
    "                wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"container__link\")))\n",
    "\n",
    "            except TimeoutException:\n",
    "                print(f\"TimeoutException occurred. Changing search term from '{search_term}' to the next one.\")\n",
    "                break\n",
    "\n",
    "            # Check if no results message is present\n",
    "            if no_results_present(driver):\n",
    "                print(f\"No results found for '{search_term}'. Changing search term to the next one.\")\n",
    "                break  # If no results, break out of the loop and change the search term\n",
    "\n",
    "            # Add a random delay between 2 to 5 seconds to simulate human-like behavior\n",
    "            delay = random.uniform(2, 5)\n",
    "            time.sleep(delay)\n",
    "\n",
    "            # Get the page source after waiting\n",
    "            page_source = driver.page_source\n",
    "\n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "            # Find all the elements with the specified class\n",
    "            links = soup.select(\".container__link.container__link--type-NewsArticle.container_list-images-with-description__link\")\n",
    "\n",
    "            # Iterate through links\n",
    "            for link in links:\n",
    "                # Get the headline\n",
    "                headline_element = link.find(\"span\", class_=\"container__headline-text\")\n",
    "                headline = headline_element.get_text() if headline_element else \"N/A\"\n",
    "\n",
    "                # Get the description\n",
    "                description_element = link.find(\"div\", class_=\"container__description\")\n",
    "                description = description_element.get_text() if description_element else \"N/A\"\n",
    "\n",
    "                # Get the date\n",
    "                date_element = link.find(\"div\", class_=\"container__date\")\n",
    "                date = date_element.get_text() if date_element else \"N/A\"\n",
    "\n",
    "                # Get the link\n",
    "                url = link.get('href')\n",
    "\n",
    "                # Check if any column contains NaN\n",
    "                if \"N/A\" in [headline, description, date, url]:\n",
    "                    continue  # Skip this row if any column contains NaN\n",
    "\n",
    "                # Check if URL already exists in CSV\n",
    "                if is_url_duplicate(csv_filename, url):\n",
    "                    continue\n",
    "\n",
    "                # Write row to CSV\n",
    "                writer.writerow({'Headline': headline.strip(), 'Description': description.strip(), 'Date': date.strip(), 'Link': url})\n",
    "                \n",
    "                # Increment the record counter\n",
    "                record_counter += 1\n",
    "                \n",
    "                # Check if 2500 records are written\n",
    "                if record_counter >= 2500:\n",
    "                    print(\"Reached 2500 records. Exiting.\")\n",
    "                    break\n",
    "\n",
    "            # Check if 2500 records are written\n",
    "            if record_counter >= 2500:\n",
    "                break\n",
    "                \n",
    "            # Add an additional random delay between 3 to 7 seconds between each page request to avoid being detected\n",
    "            delay_between_requests = random.uniform(3, 7)\n",
    "            time.sleep(delay_between_requests)\n",
    "            \n",
    "        # Check if 2500 records are written\n",
    "        if record_counter >= 2500:\n",
    "            break\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
